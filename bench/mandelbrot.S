#if 0

void
mandelbrot_rvv(size_t width, size_t maxIter, uint32_t *res)
{
	size_t VL2 = __riscv_vsetvlmax_e32m2();
	vfloat32m2_t v4 = __riscv_vfmv_v_f_f32m2(4, VL2);
	vuint32m2_t vid = __riscv_vid_v_u32m2(VL2);
	vfloat32m2_t cx, cy, zx, zy, zx2, zy2;

	for (size_t y = 0; y < width; ++y) {
		cy = __riscv_vfmv_v_f_f32m2(y, VL2);
		cy = __riscv_vfadd(__riscv_vfmul(cy, 2.0f / width, VL2), -1, VL2);

		for (size_t vl, x = 0, n = width; n > 0; n -= vl, res += vl, x += vl) {
			vl = __riscv_vsetvl_e32m2(n);

			cx = __riscv_vfcvt_f(__riscv_vadd(vid, x, vl), vl);
			cx = __riscv_vfadd(__riscv_vfmul(cx, 2.0f / width, vl), -1.5f, vl);

			size_t iter = 0;
			vuint32m2_t viter = __riscv_vmv_v_x_u32m2(0, vl);
			vbool16_t mask = __riscv_vmset_m_b16(vl);
			zx = zy = zx2 = zy2 = __riscv_vfmv_v_f_f32m2(0, vl);
			do {
				mask = __riscv_vmflt(__riscv_vfadd(zx2, zy2, vl), v4, vl);
				viter = __riscv_vadc(viter, 0, mask, vl);
				zy = __riscv_vfmacc(cy, __riscv_vfadd(zx, zx, vl), zy, vl);
				zx = __riscv_vfadd(__riscv_vfsub(zx2, zy2, vl), cx, vl);
				zx2 = __riscv_vfmul(zx, zx, vl);
				zy2 = __riscv_vfmul(zy, zy, vl);
				++iter;
			} while (iter < maxIter && __riscv_vfirst(mask, vl) >= 0);
			__riscv_vse32(res, viter, vl);
		}
	}
}

#endif

#if MX_N > 0 && MX_N <= 2

#if IF_VF16(1)+0
.global MX(mandelbrot_rvv_f16_) # generated by clang
.balign 2
MX(rvv_f16_m1p5):
	.half 0xbe00 # half -1.5
MX(rvv_f16_m1):
	.half 0xbc00 # half -1
MX(rvv_f16_p4):
	.half 0x4400 # half 4
MX(mandelbrot_rvv_f16_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e16, MX(), ta, ma
	fcvt.s.wu fa5, a0
	la a3, MX(rvv_f16_p4)
	flh fa4, (a3)
	lui a3, 262144
	fmv.w.x fa3, a3
	fdiv.s fa3, fa3, fa5
	la a3, MX(rvv_f16_m1)
	flh fa5, (a3)
	vfmv.v.f v8, fa4
	la a3, MX(rvv_f16_m1p5)
	flh fa4, (a3)
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	vsetvli zero, zero, e32, MX2(), ta, ma
	vid.v v12
	fcvt.h.s fa3, fa3
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.h.wu fa2, a6
	vsetvli a1, zero, e16, MX(), ta, ma
	vfmv.v.f v10, fa2
	vfmul.vf v10, v10, fa3
	vfadd.vf v10, v10, fa5
	mv a5, a0
	j 4f
3:
	vsetvli zero, zero, e32, MX2(), ta, ma
	vse32.v v16, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e32, MX2(), ta, ma
	vadd.vx v16, v12, a4
	vsetvli zero, zero, e16, MX(), ta, ma
	vmv.v.i v22, 0
	vfncvt.f.xu.w v20, v16
	vfmul.vf v16, v20, fa3
	vfadd.vf v20, v16, fa4
	vsetvli zero, zero, e32, MX2(), ta, ma
	vmv.v.i v16, 0
	mv a1, a7
	vmv2r.v v26, v22
	vmv2r.v v24, v22
	vmv2r.v v28, v22
5:
	vsetvli zero, zero, e16, MX(), ta, ma
	vfadd.vv v30, v26, v22
	vmflt.vv v0, v30, v8
	addi a1, a1, -1
	vsetvli zero, zero, e32, MX2(), ta, ma
	vadc.vim v16, v16, 0, v0
	beqz a1, 3b
	vsetvli zero, zero, e16, MX(), ta, ma
	vfadd.vv v28, v28, v28
	vfsub.vv v22, v26, v22
	vfmadd.vv v24, v28, v10
	vfadd.vv v28, v22, v20
	vfmul.vv v22, v24, v24
	vfirst.m a3, v0
	vfmul.vv v26, v28, v28
	bgez a3, 5b
	j 3b
9:
	ret
#endif

.global MX(mandelbrot_rvv_f32_) # generated by clang
MX(mandelbrot_rvv_f32_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e32, MX(), ta, ma
	lui a3, 264192
	fcvt.s.wu fa4, a0
	vmv.v.x v8, a3
	lui a3, 262144
	fmv.w.x fa3, a3
	lui a3, 784384
	fmv.w.x fa5, a3
	lui a3, 785408
	fdiv.s fa4, fa3, fa4
	fmv.w.x fa3, a3
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	vid.v v10
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.s.wu fa2, a6
	vsetvli a1, zero, e32, MX(), ta, ma
	vfmv.v.f v12, fa2
	vfmul.vf v12, v12, fa4
	vfadd.vf v12, v12, fa5
	mv a5, a0
	j 4f
3:
	vse32.v v14, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e32, MX(), ta, ma
	vadd.vx v14, v10, a4
	vmv.v.i v18, 0
	vfcvt.f.xu.v v14, v14
	vfmul.vf v14, v14, fa4
	vfadd.vf v16, v14, fa3
	vmv.v.i v14, 0
	mv a1, a7
	vmv.v.i v22, 0
	vmv.v.i v20, 0
	vmv.v.i v24, 0
5:
	vfadd.vv v26, v22, v18
	vmflt.vv v0, v26, v8
	addi a1, a1, -1
	vadc.vim v14, v14, 0, v0
	beqz a1, 3b
	vfadd.vv v24, v24, v24
	vfsub.vv v18, v22, v18
	vfmadd.vv v20, v24, v12
	vfadd.vv v24, v18, v16
	vfmul.vv v18, v20, v20
	vfirst.m a3, v0
	vfmul.vv v22, v24, v24
	bgez a3, 5b
	j 3b
9:
	ret

#if IF_VF64(1)+0
.balign 8
.global MX(mandelbrot_rvv_f64_) # generated by clang
MX(rvv_f64_m1p5):
	.quad 0xbff8000000000000 # double -1.5
MX(rvv_f64_m1):
	.quad 0xbff0000000000000 # double -1
MX(rvv_f64_p4):
	.quad 0x4010000000000000 # double 4
MX(mandelbrot_rvv_f64_):
	beqz a0, 9f
	li a6, 0
	vsetvli a3, zero, e64, MX(), ta, ma
	fcvt.s.wu fa5, a0
	la a3, MX(rvv_f64_p4)
	fld fa4, (a3)
	lui a3, 262144
	fmv.w.x fa3, a3
	fdiv.s fa3, fa3, fa5
	la a3, MX(rvv_f64_m1)
	fld fa5, (a3)
	vfmv.v.f v8, fa4
	la a3, MX(rvv_f64_m1p5)
	fld fa4, (a3)
	addi a3, a1, -1
	sltu a1, a1, a3
	addi a1, a1, -1
	and a1, a1, a3
	vid.v v10
	fcvt.d.s fa3, fa3
	addi a7, a1, 1
	j 2f
1:
	addi a6, a6, 1
	beq a6, a0, 9f
2:
	li a4, 0
	fcvt.d.lu fa2, a6
	vsetvli a1, zero, e64, MX(), ta, ma
	vfmv.v.f v12, fa2
	vfmul.vf v12, v12, fa3
	vfadd.vf v12, v12, fa5
	mv a5, a0
	j 4f
3:
#if HAS_RVV_1_0 || MX_N >= 2
	vsetvli zero, zero, e32, MXf2(), ta, ma
#else
	vsetvli zero, zero, e32, m1, ta, ma
#endif
	vse32.v v14, (a2)
	sub a5, a5, t0
	slli a1, t0, 2
	add a2, a2, a1
	add a4, a4, t0
	beqz a5, 1b
4:
	vsetvli t0, a5, e64, MX(), ta, ma
	vadd.vx v14, v10, a4
	vmv.v.i v18, 0
	vfcvt.f.xu.v v14, v14
	vfmul.vf v14, v14, fa3
	vfadd.vf v16, v14, fa4
#if HAS_RVV_1_0 || MX_N >= 2
	vsetvli zero, zero, e32, MXf2(), ta, ma
#else
	vsetvli zero, zero, e32, m1, ta, ma
#endif
	vmv.v.i v14, 0
	mv a1, a7
	vmv2r.v v22, v18
	vmv2r.v v20, v18
	vmv2r.v v24, v18
5:
	vsetvli zero, zero, e64, MX(), ta, ma
	vfadd.vv v26, v22, v18
	vmflt.vv v0, v26, v8
	addi a1, a1, -1
#if HAS_RVV_1_0 || MX_N >= 2
	vsetvli zero, zero, e32, MXf2(), ta, ma
#else
	vsetvli zero, zero, e32, m1, ta, ma
#endif
	vadc.vim v14, v14, 0, v0
	beqz a1, 3b
	vsetvli zero, zero, e64, MX(), ta, ma
	vfadd.vv v24, v24, v24
	vfsub.vv v18, v22, v18
	vfmadd.vv v20, v24, v12
	vfadd.vv v24, v18, v16
	vfmul.vv v18, v20, v20
	vfirst.m a3, v0
	vfmul.vv v22, v24, v24
	bgez a3, 5b
	j 3b
9:
	ret

#endif

#endif


